<h1>Speech-to-Text (S2T)</h1>
<p>Speech2Text (Speech-to-Text, Automatic Speech Recognition, распознавание речи) - технология, которая отвечает за перевод устной речи в текст.</p>
<p>В её основе — многоуровневый процесс, который включает обработку и анализ аудио. Речь с помощью искусственного интеллекта преобразуется сначала в буквы, затем слова, фразы, предложения, и в результате получается текст.</p>
<p>Системы распознавания речи бывают двух видов – гибридные и end2end. End2end сразу переводят последовательность звуков в последовательность букв, гибридные содержат акустическую и языковую модель, работающие независимо. С конца 2010-х основным методом для распознавания речи стали end-to-end-модели.</p>
<h2>Архитектуры S2T</h2>
<p>Процесс распознавания речи основан на двух моделях: акустической (преобразовывает речь в буквы) и языковой (анализирует контекст, выбирая наиболее вероятное слово). В end-to-end-подходе основную часть работы выполняет акустическая модель. Обычно она состоит из энкодера, который преобразует звук в представления, понятные модели (чаще всего используются CNN и Transformers), и декодера, который с помощью этих представлений генерирует текст.</p>
<p>Существует три основных архитектуры для end-to-end распознавания речи:</p>
<h3>1. CTC (Connectionist Temporal Classification)</h3>
<p>CTC позволяет модели предсказывать последовательности без явного выравнивания между входом (аудио) и выходом (текстом). Это особенно полезно, когда длина выходной последовательности меньше длины входа, а соответствие между элементами неизвестно заранее.</p>
<p><strong>Формула потерь (CTC Loss):</strong>
[
\mathcal{L}_{CTC} = - \log \sum_{\pi \in B^{-1}(y)} P(\pi | x)
]
Где:</p>
<ul>
<li>( \pi ) — допустимые выравнивания,</li>
<li>( B^{-1}(y) ) — множество выравниваний, сворачивающихся в целевую последовательность ( y ),</li>
<li>( x ) — входная последовательность (например, спектрограмма).</li>
</ul>
<p><strong>Плюсы:</strong> быстрый, подходит для больших наборов данных.
<strong>Минусы:</strong> плохо справляется с длинными контекстами и неоднозначностями.</p>
<h3>2. RNN Transducer (RNN-T)</h3>
<p>Комбинирует энкодер, предсказатель и модуль объединения (joiner). Подходит для стриминга, так как обрабатывает вход по мере поступления.</p>
<p><strong>Формула:</strong>
[
P(y | x) = \prod_{t=1}^T P(y_t | x_{\le t}, y_{&lt;t})
]</p>
<p><strong>Преимущество:</strong> учитывает предыдущие символы (авто-регрессия).
<strong>Недостаток:</strong> сложность обучения и вычислительная нагрузка.</p>
<h3>3. LAS (Listen, Attend and Spell)</h3>
<p>Encoder-Decoder модель с attention-механизмом. Подходит для оффлайн обработки, эффективно работает с длинными аудиофрагментами.</p>
<p><strong>Проблема:</strong> при длинных входах внимание может «размываться» — становится сложнее сфокусироваться на нужной части сигнала, особенно в записях с шумами и паузами.</p>
<h3>Сравнение</h3>
<table>
<thead>
<tr>
<th>Архитектура</th>
<th>Потоковая обработка</th>
<th>Учет контекста</th>
<th>Простота</th>
<th>Качество</th>
</tr>
</thead>
<tbody>
<tr>
<td>CTC</td>
<td>✅</td>
<td>❌</td>
<td>✅</td>
<td>среднее</td>
</tr>
<tr>
<td>RNN-T</td>
<td>✅</td>
<td>✅</td>
<td>❌</td>
<td>высокое</td>
</tr>
<tr>
<td>LAS</td>
<td>❌</td>
<td>✅ (вся история)</td>
<td>❌</td>
<td>высокое</td>
</tr>
</tbody>
</table>
<h3>Обработка текста после распознавания</h3>
<p>После получения текстовой последовательности применяется нормализация:</p>
<ul>
<li>восстановление пунктуации;</li>
<li>преобразование чисел в читаемую форму;</li>
<li>коррекция регистра.</li>
</ul>
<h2>Современные модели</h2>
<ul>
<li><strong>Whisper (OpenAI)</strong> — трансформер, обученный на 680 тыс. часов данных. Особенность — мультизадачность: перевод, сегментация, диаризация.</li>
<li><strong>Wav2Vec 2.0 (Meta AI)</strong> — pretraining без разметки (self-supervised), затем дообучение с CTC. Особенно эффективен при ограниченных размеченных данных.</li>
</ul>
<h2>Общий пайплайн</h2>
<ol>
<li>Преобразование аудио в мел-спектрограмму.</li>
<li>Прогон через энкодер (CNN, Transformer).</li>
<li>Декодирование с CTC, RNN-T или attention.</li>
<li>Постобработка текста.</li>
</ol>
<h2>Проблема данных и краудсорсинг</h2>
<p>Речевые датасеты — дорогие и трудоёмкие в сборе. Чтобы решить проблему, используют <strong>краудсорсинг</strong>: пользователи помогают размечать аудио — дешево, быстро, масштабируемо. Это позволяет собирать большие корпуса разговорной, диалектной и специализированной речи.</p>
<h2>Инструменты и библиотеки</h2>
<ul>
<li><a href="https://pytorch.org/audio/">torchaudio</a></li>
<li><a href="https://github.com/espnet/espnet">ESPnet</a></li>
<li><a href="https://github.com/openai/whisper">Whisper</a></li>
<li><a href="https://kaldi-asr.org/">Kaldi</a></li>
<li><a href="https://commonvoice.mozilla.org/ru">Common Voice</a> — краудсорсинговый датасет</li>
</ul>
<hr>
<h1>Text-to-Speech (T2S)</h1>
<p>Text2Speech (Text-to-Speech, Text-to-Voice, генерация речи, синтез речи и т.п.) - технология, которая преобразует написанный текст в аудиосигнал.</p>
<p>Есть много методов синтеза речи, но наибольшее значение имеют две группы технологий: конкатенация (unit selection) и синтез по параметрам с использованием глубоких нейронных сетей.</p>
<p>При конкатенативном подходе синтезатор голоса использует предварительно записанные звуки, слова и фразы. Синтезированная речь получается довольно качественной, хотя и с некоторыми недостатками: монотонностью и артефактами на стыке фрагментов. С конца 90-х долгое время этот метод считался стандартом для разработки движков синтеза речи. Например, голос, звучащий по методу unit selection, можно было встретить в Siri.</p>
<p>Параметрический синтез на основе глубокого обучения стал активно развиваться примерно с 2016 года. Его суть состоит в построении вероятностной модели, предсказывающей акустические свойства аудиосигнала для данного текста. Сегодня созданная этим методом речь практически не отличается от натуральной.</p>
<p>Речь моделей unit selection имеет высокое качество, низкую вариативность и требует большого объема данных для обучения. В то же время для тренировки параметрических моделей необходимо гораздо меньшее количество данных, они генерируют более разнообразные интонации, но до недавнего времени страдали от общего достаточно низкого качества звука по сравнению с подходом unit selection. Сейчас нейросети окончательно заменили старые методы синтеза речи.</p>
<h2>Этапы синтеза речи</h2>
<ol>
<li><strong>Нормализация текста</strong> — раскрытие чисел, аббревиатур, постановка ударений.</li>
<li><strong>Text Encoding</strong> — преобразование текста в эмбеддинги (CNN, RNN, Transformer).</li>
<li><strong>Duration Prediction</strong> — сколько времени звучит каждая фонема (можно обучать по выравниванию).</li>
<li><strong>Mel-Spectrogram Prediction</strong> — генерация визуального представления звука.</li>
<li><strong>Вокодинг</strong> — превращение спектрограммы в звуковую волну.</li>
</ol>
<h2>Популярные архитектуры</h2>
<h3>Tacotron 2</h3>
<ul>
<li>Использует attention-модель</li>
<li>Сильно зависит от порядка и пунктуации</li>
<li>Вокодер: WaveGlow</li>
</ul>
<h3>FastSpeech 2</h3>
<ul>
<li>Устраняет нестабильности Tacotron за счёт duration prediction</li>
<li>Более быстрый и стабильный</li>
</ul>
<h3>VITS</h3>
<ul>
<li>End-to-End архитектура: синтез без промежуточной спектрограммы</li>
<li>GAN + VAE + Flows — один из самых естественных синтезаторов</li>
</ul>
<h3>Grad-TTS</h3>
<ul>
<li>Диффузионная модель</li>
<li>Генерация спектрограммы из шума</li>
</ul>
<h2>Примеры потерь</h2>
<ul>
<li><strong>MSE по спектрограмме:</strong>
[
\mathcal{L}_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^n (\hat{s}_i - s_i)^2
]</li>
<li><strong>Duration Loss</strong> — для длины фонем</li>
<li><strong>Adversarial Loss</strong> (для GAN):
[
\mathcal{L}_{\text{adv}} = \mathbb{E}_{x} [\log D(x)] + \mathbb{E}_{\hat{x}} [\log(1 - D(\hat{x}))]
]</li>
</ul>
<h2>Современные модели</h2>
<ul>
<li><a href="https://github.com/NVIDIA/tacotron2">Tacotron2</a></li>
<li><a href="https://github.com/ming024/FastSpeech2">FastSpeech2</a></li>
<li><a href="https://github.com/jaywalnut310/vits">VITS</a></li>
<li><a href="https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS">Grad-TTS</a></li>
</ul>
<h2>Метрики оценки</h2>
<ul>
<li><strong>MOS (Mean Opinion Score)</strong> — субъективная оценка (от 1 до 5)</li>
<li><strong>Mel Cepstral Distortion (MCD)</strong> — сравнение спектров</li>
<li><strong>Word Error Rate (для T2S→ASR)</strong> — через обратное распознавание</li>
</ul>
<p><strong>Формула WER:</strong>
[
WER = \frac{S + D + I}{N}
]
Где S — замены, D — удаления, I — вставки, N — количество слов в эталоне.</p>
<h2>Библиотеки</h2>
<ul>
<li><a href="https://librosa.org/">librosa</a></li>
<li><a href="https://pytorch.org/audio/">torchaudio</a></li>
<li><a href="https://github.com/google/REAPER">reaper</a></li>
</ul>
