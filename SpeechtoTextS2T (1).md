# Speech-to-Text (S2T)

Speech2Text (Speech-to-Text, Automatic Speech Recognition, распознавание речи) - технология, которая отвечает за перевод устной речи в текст.

В её основе — многоуровневый процесс, который включает обработку и анализ аудио. Речь с помощью искусственного интеллекта преобразуется сначала в буквы, затем слова, фразы, предложения, и в результате получается текст.

Системы распознавания речи бывают двух видов – гибридные и end2end. End2end сразу переводят последовательность звуков в последовательность букв, гибридные содержат акустическую и языковую модель, работающие независимо. С конца 2010-х основным методом для распознавания речи стали end-to-end-модели.

## Архитектуры S2T

Процесс распознавания речи основан на двух моделях: акустической (преобразовывает речь в буквы) и языковой (анализирует контекст, выбирая наиболее вероятное слово). В end-to-end-подходе основную часть работы выполняет акустическая модель. Обычно она состоит из энкодера, который преобразует звук в представления, понятные модели (чаще всего используются CNN и Transformers), и декодера, который с помощью этих представлений генерирует текст. 

Существует три основных архитектуры для end-to-end распознавания речи:

### 1. CTC (Connectionist Temporal Classification)

CTC позволяет модели предсказывать последовательности без явного выравнивания между входом (аудио) и выходом (текстом). Это особенно полезно, когда длина выходной последовательности меньше длины входа, а соответствие между элементами неизвестно заранее.

**Формула потерь (CTC Loss):**
\[
\mathcal{L}_{CTC} = - \log \sum_{\pi \in B^{-1}(y)} P(\pi | x)
\]
Где:
- \( \pi \) — допустимые выравнивания,
- \( B^{-1}(y) \) — множество выравниваний, сворачивающихся в целевую последовательность \( y \),
- \( x \) — входная последовательность (например, спектрограмма).

**Плюсы:** быстрый, подходит для больших наборов данных.
**Минусы:** плохо справляется с длинными контекстами и неоднозначностями.

### 2. RNN Transducer (RNN-T)

Комбинирует энкодер, предсказатель и модуль объединения (joiner). Подходит для стриминга, так как обрабатывает вход по мере поступления.

**Формула:**
\[
P(y | x) = \prod_{t=1}^T P(y_t | x_{\le t}, y_{<t})
\]

**Преимущество:** учитывает предыдущие символы (авто-регрессия).
**Недостаток:** сложность обучения и вычислительная нагрузка.

### 3. LAS (Listen, Attend and Spell)

Encoder-Decoder модель с attention-механизмом. Подходит для оффлайн обработки, эффективно работает с длинными аудиофрагментами.

**Проблема:** при длинных входах внимание может «размываться» — становится сложнее сфокусироваться на нужной части сигнала, особенно в записях с шумами и паузами.

### Сравнение
| Архитектура | Потоковая обработка | Учет контекста | Простота | Качество |
|------------|----------------------|----------------|----------|----------|
| CTC        | ✅                   | ❌             | ✅        | среднее  |
| RNN-T      | ✅                   | ✅             | ❌        | высокое  |
| LAS        | ❌                   | ✅ (вся история) | ❌        | высокое  |

### Обработка текста после распознавания
После получения текстовой последовательности применяется нормализация:
- восстановление пунктуации;
- преобразование чисел в читаемую форму;
- коррекция регистра.

## Современные модели

- **Whisper (OpenAI)** — трансформер, обученный на 680 тыс. часов данных. Особенность — мультизадачность: перевод, сегментация, диаризация.
- **Wav2Vec 2.0 (Meta AI)** — pretraining без разметки (self-supervised), затем дообучение с CTC. Особенно эффективен при ограниченных размеченных данных.

## Общий пайплайн
1. Преобразование аудио в мел-спектрограмму.
2. Прогон через энкодер (CNN, Transformer).
3. Декодирование с CTC, RNN-T или attention.
4. Постобработка текста.

## Проблема данных и краудсорсинг

Речевые датасеты — дорогие и трудоёмкие в сборе. Чтобы решить проблему, используют **краудсорсинг**: пользователи помогают размечать аудио — дешево, быстро, масштабируемо. Это позволяет собирать большие корпуса разговорной, диалектной и специализированной речи.

## Инструменты и библиотеки
- [torchaudio](https://pytorch.org/audio/)
- [ESPnet](https://github.com/espnet/espnet)
- [Whisper](https://github.com/openai/whisper)
- [Kaldi](https://kaldi-asr.org/)
- [Common Voice](https://commonvoice.mozilla.org/ru) — краудсорсинговый датасет

---

# Text-to-Speech (T2S)

Text2Speech (Text-to-Speech, Text-to-Voice, генерация речи, синтез речи и т.п.) - технология, которая преобразует написанный текст в аудиосигнал.

Есть много методов синтеза речи, но наибольшее значение имеют две группы технологий: конкатенация (unit selection) и синтез по параметрам с использованием глубоких нейронных сетей.

При конкатенативном подходе синтезатор голоса использует предварительно записанные звуки, слова и фразы. Синтезированная речь получается довольно качественной, хотя и с некоторыми недостатками: монотонностью и артефактами на стыке фрагментов. С конца 90-х долгое время этот метод считался стандартом для разработки движков синтеза речи. Например, голос, звучащий по методу unit selection, можно было встретить в Siri.

Параметрический синтез на основе глубокого обучения стал активно развиваться примерно с 2016 года. Его суть состоит в построении вероятностной модели, предсказывающей акустические свойства аудиосигнала для данного текста. Сегодня созданная этим методом речь практически не отличается от натуральной.

Речь моделей unit selection имеет высокое качество, низкую вариативность и требует большого объема данных для обучения. В то же время для тренировки параметрических моделей необходимо гораздо меньшее количество данных, они генерируют более разнообразные интонации, но до недавнего времени страдали от общего достаточно низкого качества звука по сравнению с подходом unit selection. Сейчас нейросети окончательно заменили старые методы синтеза речи.

## Этапы синтеза речи

1. **Нормализация текста** — раскрытие чисел, аббревиатур, постановка ударений.
2. **Text Encoding** — преобразование текста в эмбеддинги (CNN, RNN, Transformer).
3. **Duration Prediction** — сколько времени звучит каждая фонема (можно обучать по выравниванию).
4. **Mel-Spectrogram Prediction** — генерация визуального представления звука.
5. **Вокодинг** — превращение спектрограммы в звуковую волну.

## Популярные архитектуры

### Tacotron 2

- Использует attention-модель
- Сильно зависит от порядка и пунктуации
- Вокодер: WaveGlow

### FastSpeech 2

- Устраняет нестабильности Tacotron за счёт duration prediction
- Более быстрый и стабильный

### VITS

- End-to-End архитектура: синтез без промежуточной спектрограммы
- GAN + VAE + Flows — один из самых естественных синтезаторов

### Grad-TTS

- Диффузионная модель
- Генерация спектрограммы из шума

## Примеры потерь
- **MSE по спектрограмме:**
\[
\mathcal{L}_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^n (\hat{s}_i - s_i)^2
\]
- **Duration Loss** — для длины фонем
- **Adversarial Loss** (для GAN):
\[
\mathcal{L}_{\text{adv}} = \mathbb{E}_{x} [\log D(x)] + \mathbb{E}_{\hat{x}} [\log(1 - D(\hat{x}))]
\]

## Современные модели
- [Tacotron2](https://github.com/NVIDIA/tacotron2)
- [FastSpeech2](https://github.com/ming024/FastSpeech2)
- [VITS](https://github.com/jaywalnut310/vits)
- [Grad-TTS](https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS)

## Метрики оценки
- **MOS (Mean Opinion Score)** — субъективная оценка (от 1 до 5)
- **Mel Cepstral Distortion (MCD)** — сравнение спектров
- **Word Error Rate (для T2S→ASR)** — через обратное распознавание

**Формула WER:**
\[
WER = \frac{S + D + I}{N}
\]
Где S — замены, D — удаления, I — вставки, N — количество слов в эталоне.

## Библиотеки
- [librosa](https://librosa.org/)
- [torchaudio](https://pytorch.org/audio/)
- [reaper](https://github.com/google/REAPER)